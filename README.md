# ğŸ¤– Autonomous Mobile Manipulation System â€“ Group Project Fork

A fork of a collaborative robotics project focused on mobile manipulation, perception, and task execution in a semi-structured environment.  
I was responsible for and contributed to the following components:

---

## ğŸ” Perception & Object Detection
- ğŸ“¦ Implemented deep learningâ€“based object detection using Intel RealSense RGB-D data  
- ğŸ§  Integrated ArUco marker detection for precise localization of objects and boxes  
- ğŸ” Designed object de-duplication via bounding box distance thresholding  

---

## ğŸ¤– Arm Control & Grasping
- ğŸ¦¾ Developed a vision-based servoing pipeline for the robotic arm (no IK required)  
- ğŸ¯ Implemented closed-loop control using HSV color filtering & edge detection  
- ğŸ§¸ Designed dual-strategy grasping for cube/sphere (color) and fluffy objects (edge + DL)  
- âš™ï¸ Built a Cartesian controller with PI control loop for smooth motor coordination  
- ğŸ›ï¸ Controlled multi-axis motors (ID 3â€“6) with centroid-guided tracking and yaw alignment  

---

## ğŸ“ Localization & Navigation
- ğŸ§­ Integrated ICP-based localization with wheel odometry fusion  
- ğŸ—ºï¸ Contributed to multi-sensor odometry and TF transform debugging  
- ğŸ§± Supported A* path planning and waypoint-based trajectory following  

---

## ğŸ§  System Integration & Strategy
- ğŸ§© Contributed to the final behavior tree logic and goal/task switching  
- ğŸ® Developed joystick teleoperation interface for manual override and debugging  
- ğŸ”— Integrated all modules: detection, localization, planning, arm control, and system orchestration  

---

## ğŸ› ï¸ Technologies Used
- ROS (Robot Operating System)
- Python & C++
- OpenCV, TensorFlow/PyTorch (for DL detection)
- RealSense SDK, ArUco, LiDAR, Phidgets Motor Controller
